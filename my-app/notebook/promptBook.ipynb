{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Playground for prompt engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install openai\n",
    "%pip install --upgrade tiktoken\n",
    "%pip install PyJSONViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv # load keys from .env file\n",
    "import openai # use OpenAI API\n",
    "import tiktoken # use OpenAI token counter API\n",
    "import wolframalpha # use Wolfram Alpha API\n",
    "import random\n",
    "\n",
    "import json\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    print(\">>>>\")\n",
    "    print(str(response.choices[0].message[\"content\"]))\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    print(str(response.choices[0].message))\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>\n",
      "1. Limited reasoning capabilities: Language models are designed to understand and generate human-like text based on patterns they have learned from large datasets. They do not possess the ability to reason or think analytically like humans do. Their responses are based on statistical associations rather than logical reasoning.\n",
      "\n",
      "2. Lack of domain-specific knowledge: Language models may not have in-depth knowledge of specific domains or subjects, which is often required for analytical thinking and problem-solving tasks. They can only provide information that they have been trained on, and may not be able to generate accurate or relevant responses for specialized topics.\n",
      "\n",
      "3. Inability to handle complex calculations: Language models are not designed to perform complex calculations or mathematical operations, which are often required in analytical tasks. They may struggle to provide accurate results for problems that involve numerical data or require mathematical reasoning.\n",
      "\n",
      "4. Difficulty in understanding context: Language models may not always understand the context of a given problem or task, leading to irrelevant or incorrect responses. Analytical thinking often requires understanding the nuances and context of a situation to provide accurate and meaningful solutions.\n",
      "\n",
      "5. No structured approach: Language models generate responses based on patterns and associations in the data they have been trained on, rather than following a structured approach to problem-solving. Analytical tasks often require a systematic and structured approach to arrive at a solution, which language models may not be able to provide.\n",
      "\n",
      "6. Prone to biases: Language models can inherit biases present in the training data, which may affect their ability to provide objective and unbiased solutions to analytical tasks. This can lead to incorrect or inappropriate responses that may not be suitable for certain tasks.\n",
      "\n",
      "7. Inability to learn from feedback: Unlike humans, language models cannot learn from feedback or adapt their approach to problem-solving based on new information. This limits their ability to improve their performance in analytical tasks over time.\n",
      "\n",
      "In summary, while language models are powerful tools for natural language tasks, they are not suitable for tasks that demand analytical thinking with a structured approach due to their limited reasoning capabilities, lack of domain-specific knowledge, inability to handle complex calculations, difficulty in understanding context, lack of a structured approach, proneness to biases, and inability to learn from feedback.\n",
      "1. Limited reasoning capabilities: Language models are designed to understand and generate human-like text based on patterns they have learned from large datasets. They do not possess the ability to reason or think analytically like humans do. Their responses are based on statistical associations rather than logical reasoning.\n",
      "\n",
      "2. Lack of domain-specific knowledge: Language models may not have in-depth knowledge of specific domains or subjects, which is often required for analytical thinking and problem-solving tasks. They can only provide information that they have been trained on, and may not be able to generate accurate or relevant responses for specialized topics.\n",
      "\n",
      "3. Inability to handle complex calculations: Language models are not designed to perform complex calculations or mathematical operations, which are often required in analytical tasks. They may struggle to provide accurate results for problems that involve numerical data or require mathematical reasoning.\n",
      "\n",
      "4. Difficulty in understanding context: Language models may not always understand the context of a given problem or task, leading to irrelevant or incorrect responses. Analytical thinking often requires understanding the nuances and context of a situation to provide accurate and meaningful solutions.\n",
      "\n",
      "5. No structured approach: Language models generate responses based on patterns and associations in the data they have been trained on, rather than following a structured approach to problem-solving. Analytical tasks often require a systematic and structured approach to arrive at a solution, which language models may not be able to provide.\n",
      "\n",
      "6. Prone to biases: Language models can inherit biases present in the training data, which may affect their ability to provide objective and unbiased solutions to analytical tasks. This can lead to incorrect or inappropriate responses that may not be suitable for certain tasks.\n",
      "\n",
      "7. Inability to learn from feedback: Unlike humans, language models cannot learn from feedback or adapt their approach to problem-solving based on new information. This limits their ability to improve their performance in analytical tasks over time.\n",
      "\n",
      "In summary, while language models are powerful tools for natural language tasks, they are not suitable for tasks that demand analytical thinking with a structured approach due to their limited reasoning capabilities, lack of domain-specific knowledge, inability to handle complex calculations, difficulty in understanding context, lack of a structured approach, proneness to biases, and inability to learn from feedback.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "myPrompt = \"while a language model is suitable for natural language tasks, it is not suitable for tasks that demand analytical thinking with a structured approach because ...\"\n",
    "print(get_completion(myPrompt, model=\"gpt-4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.28e+09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = (600 / 2 * 30000 * 365)\n",
    "print(\"{:.2e}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
    "{'role':'user', 'content':'tell me a joke'},   \n",
    "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
    "{'role':'user', 'content':'I don\\'t know'}  ]\n",
    "\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fermi Estimation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"How tall, in feet, is the 22nd tallest building in the world?\"\n",
    "# ask LLM to give its estimation chain of thought.\n",
    "prompt_reason = f\"\"\"\n",
    "You are an AI assistant helping people practice Fermi estimation techniques.\\\n",
    "A human will post a question and ask for help to estimate a quantity in the order of magnitude.\\\n",
    "The estimation should rely on human's common senses and knowledge about the world.\n",
    "\n",
    "Please reason through the following steps before generate your output.\n",
    "1. Identify the final quantity of interest, T, or the target quantity, that needs to be estimated.\\\n",
    "2. Creatively generate a list of factors that affect the target quantity, and the any relevant sub-quantities.\\\n",
    "3. Logically link relevant factors together to create a mathematical model.\n",
    "4. Make sure the units are coherent in your model.\n",
    "\n",
    "\n",
    "The question is delimited with triple backticks.\n",
    "Question: ```{question}```\n",
    "Reason: [INSERT REASONING]\n",
    "Answer: [INSERT FINAL ANSWER]\n",
    "\"\"\"\n",
    "response = get_completion(prompt_reason)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the mass of Mars in iPhone 5s?\"\n",
    "# ask LLM to give its estimation chain of thought.\n",
    "prompt_reason = f\"\"\"\n",
    "You are an AI assistant helping people practice Fermi estimation techniques.\\\n",
    "A human will post a question and ask for help to estimate a quantity inquired by the question in the order of magnitude.\\\n",
    "The estimation should rely on human's common senses and knowledge about the world.\n",
    "\n",
    "Please reason through the following steps before generate your output.\n",
    "1. Identify the final quantity of interest, T, or the target quantity, that needs to be estimated.\\\n",
    "2. Creatively generate a list of factors that affect the target quantity, and the any relevant sub-quantities.\\\n",
    "3. Logically link relevant factors together to create a mathematical model.\n",
    "4. Make sure the units are coherent in your model.\n",
    "\n",
    "\n",
    "The question is delimited with triple backticks.\n",
    "Question: ```{question}```\n",
    "Reason: [INSERT REASONING]\n",
    "Answer: [INSERT FINAL ANSWER]\n",
    "\"\"\"\n",
    "response = get_completion(prompt_reason)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask LLM to give its estimation chain of thought.\n",
    "prompt_wolfram = f\"\"\"\n",
    "\n",
    "I want to infer the volumn of a cylinder with a height of 5 meter and radius of 30cm. Help me right a prompt for Wolfram Alpha web api.\n",
    "\n",
    "\"\"\"\n",
    "response1 = get_completion(prompt_wolfram,model=\"gpt-3.5-turbo\")\n",
    "print(response1)\n",
    "response2 = get_completion(prompt_wolfram,model=\"gpt-4-0314\")\n",
    "print(response2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalised Estimation Q&A Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: classify the question as a Fermi Question\n",
    "question = \"Despite what you may have learned in chemistry class, any pair of atoms will stick to each other to form a diatomic molecule. Given this fact, how many species of diatomic molecules are there?\"\n",
    "prompt_1 = f\"\"\"\n",
    "\n",
    "You are an AI assistant helping people practice Fermi estimation techniques.\n",
    "A curious user will ask you an estimation question after your see the delimitators ===.\n",
    "A Fermi Question requires a quick, estimated answer which would be impractical to measure directly.\n",
    "Your task is to classify if the user input is a valid Fermi Question or Fermi Problem.\n",
    "\n",
    "Follow the examples to determine if the user input is a qualified Fermi Question.\n",
    "\n",
    "User: How much does the Thames River heat up in going over the Fanshawe Dam? (Celsius degrees).\n",
    "Answer: true\n",
    "\n",
    "User: What is the mass of all the automobiles scrapped in North America this month? (kilograms).\"\n",
    "Answer: true\n",
    "\n",
    "User: What's the longer river on earth?\n",
    "Answer: false\n",
    "\n",
    "User: What's the probability of tomorrow being a sunny day?\n",
    "Answer: false\n",
    "\n",
    "===\n",
    "User: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "print(\"ROUND0 - CLASSIFICATION\")\n",
    "response1 = get_completion(prompt_1,model=\"gpt-3.5-turbo\")\n",
    "response2 = get_completion(prompt_1,model=\"gpt-4-0314\")\n",
    "\n",
    "# KNOWN issue:\n",
    "# for this question (with more context):\n",
    "# Despite what you may have learned in chemistry class, any pair of atoms will stick to each other to form a diatomic molecule. \n",
    "# Given this fact, how many species of diatomic molecules are there?\n",
    "\n",
    "# GPT3.5 classify as true while GPT4 classify as false -> how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: ask LLM to 'understand' the input question first.\n",
    "question = \"How many ice creams are consumed in London city?\"\n",
    "prompt_1 = f\"\"\"\n",
    "\n",
    "You are an AI assistant helping people practice Fermi estimation techniques.\\\n",
    "A curious user will ask you an estimation question and test if you understand the meaning of the question. \\\n",
    "You will respond by paraphrase the question into a statement. \\\n",
    "For example, if the user ask: what is the number of piano tuners in Chicago? \\\n",
    "Your response will be: You'd like to estimate the number of piano tuners in the city of Chicago, the USA.\n",
    "If you don't understand the question, just respond: \"Sorry I don't fully understand the question. Could you please clarify?\" \\\n",
    "If the question is not relevant to quantity estimation, respond with \"Sorry, but I cannot answer that question.\" \\\n",
    "Respond with maximum 3 sentences.\n",
    "\n",
    "User: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "print(\"ROUND1 - CLASSIFICATION\")\n",
    "response1 = get_completion(prompt_1,model=\"gpt-3.5-turbo\")\n",
    "print(\"GPT3.5: \" + response1)\n",
    "response2 = get_completion(prompt_1,model=\"gpt-4-0314\")\n",
    "print(\"GPT4: \" + response2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: once the answer is accepted, continue with information extraction\n",
    "def prompt_2(original_question, model_response):\n",
    "    return f\"\"\"\n",
    "        A curious user asked an estimation question and you responded with your interpretation. \\\n",
    "        The user asked: {original_question}\n",
    "        You responded: {model_response}\n",
    "        Now will extract information by identifying the final quantify of interest,\\\n",
    "        and what factors this quantity is dependent on. Do not give numerical estimation yet.\n",
    "        Respond with maximum 5 sentences.\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "prompt2_1 = prompt_2(question, response1)\n",
    "prompt2_2 = prompt_2(question, response2)\n",
    "print(\"ROUND2 - CLASSIFICATION\")\n",
    "response2_1 = get_completion(prompt2_1,model=\"gpt-3.5-turbo\")\n",
    "print(\"GPT3.5: \" + response2_1)\n",
    "response2_2 = get_completion(prompt2_2,model=\"gpt-4-0314\")\n",
    "print(\"GPT4: \" + response2_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUND2 - CLASSIFICATION\n",
    ">>>>\n",
    "GPT3.5: The final quantity of interest is the width of the green door on Mars. This quantity is dependent on several factors such as the existence of a green door on Mars, the location of the door, the method of measurement, and the accuracy of the measurement tool. Additionally, the width of the door may vary depending on its purpose and design. Without further information, it is difficult to provide a precise estimation of the width of the green door on Mars.\n",
    ">>>>\n",
    "GPT4: The final quantity of interest in this estimation question is the width of a green door on Mars. This quantity would depend on factors such as the purpose of the door, the size of the structure it is attached to, and the design preferences of the creators. Additionally, the width may be influenced by the materials used to construct the door and any specific requirements for the Martian environment, such as pressure differences or temperature fluctuations. Lastly, the width could also be affected by any potential Martian inhabitants' size and accessibility needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT3.5: The final quantity of interest is the proportion of Earth's water supply that is held within living organisms. This quantity is dependent on various factors such as the total amount of water on Earth, the distribution of water across different ecosystems, and the size and number of living organisms. It is also influenced by the water requirements and water retention capabilities of different species. Understanding this proportion can provide insights into the role of living organisms in the global water cycle and the impact of human activities on water resources. However, obtaining an accurate estimate of this proportion can be challenging due to the complexity and variability of ecosystems and the lack of comprehensive data.\n",
    "\n",
    "GPT4: The final quantity of interest is the fraction of Earth's total water supply contained within all living beings. This quantity depends on factors such as the total volume of water on Earth, the biomass of all living organisms, and the average water content within these organisms. To estimate this fraction, we would need to gather data on the global water distribution, the total biomass of various life forms, and their respective water content percentages. By combining this information, we can calculate the total amount of water contained within living beings and compare it to the Earth's total water supply. This will give us an estimation of the fraction of water contained within all forms of life on our planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: tabulate\n",
    "def prompt_3(original_question, model_response_2):\n",
    "    return f\"\"\"\n",
    "        A curious user asked an estimation question and you responded with your interpretation. \\\n",
    "        The user asked: {original_question} \\\n",
    "        You identified final quantity of interest and what factors or other quantities it depends on. \\\n",
    "        Now you will extract all relevant quantities from your response into a Markdown table: {model_response_2}. \\\n",
    "        For example:\\\n",
    "        | Quantity | value | unit |\\\n",
    "        |-----|-----|-----|\\\n",
    "        |[final quantity of interest] |[Leave this bank]|[unit based on the question]|\\\n",
    "        |[relevant quantity A] |[Leave this bank] |[unit based on the question] |\\\n",
    "        |...|...|...|\\\n",
    "        \n",
    "        Answer with only markdown table and no conversations.\n",
    "        ---\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "prompt3_1 = prompt_3(question, response2_1)\n",
    "prompt3_2 = prompt_3(question, response2_2)\n",
    "print(\"ROUND3 - MAKE TABLE\")\n",
    "response3_1 = get_completion(prompt3_1,model=\"gpt-3.5-turbo\")\n",
    "print(\"GPT3.5: \" + \"\\n\" + response3_1)\n",
    "response3_2 = get_completion(prompt3_2,model=\"gpt-4-0314\")\n",
    "print(\"GPT4: \" + \"\\n\" + response3_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Quantity | value | unit |\n",
    "|-----|-----|-----|\n",
    "|[final quantity of interest] |[Leave this bank]|[unit based on the question]|\n",
    "|[population of the city] |[Leave this bank] |[unit based on the question] |\n",
    "|[weather conditions] |[Leave this bank] |[unit based on the question] |\n",
    "|[availability of ice cream vendors] |[Leave this bank] |[unit based on the question] |\n",
    "|[time of year] |[Leave this bank] |[unit based on the question] |\n",
    "|[cultural preferences] |[Leave this bank] |[unit based on the question] |\n",
    "|[age demographics] |[Leave this bank] |[unit based on the question] |\n",
    "|[economic factors] |[Leave this bank] |[unit based on the question] |\n",
    "\n",
    "GPT4:\n",
    "| Quantity                          | Value | Unit                  |\n",
    "|-----------------------------------|-------|-----------------------|\n",
    "| Final Quantity of Interest        |       | Ice creams consumed   |\n",
    "| Population of London              |       | People                |\n",
    "| Percentage of Ice Cream Consumers |       | Percentage            |\n",
    "| Frequency of Consumption          |       | Times per week/month  |\n",
    "| Seasonal Variations               |       | Ice creams per season |\n",
    "| Number of Tourists                |       | People                |\n",
    "| Tourist Ice Cream Consumption     |       | Ice creams per tourist|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: From Table to JSON\n",
    "import json\n",
    "example_json = {\n",
    "  \"name\": \"root\",\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"child1\",\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"child1.1\",\n",
    "          \"children\": []\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"child2\",\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"child2.1\",\n",
    "          \"children\": []\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "example = json.dumps(example_json)\n",
    "def prompt_4(original_question, model_response_table, example=example):\n",
    "    return f\"\"\"\n",
    "        A curious user asked an estimation question and you responded with your interpretation. \\\n",
    "        The user asked: {original_question} \\\n",
    "        You identified final quantity of interest and what factors or other quantities it depends on. \\\n",
    "        Then you extract all relevant quantities into a Markdown table: \\\n",
    "        {model_response_table}. \\\n",
    "        Now you will transform the information from the table into a nested JSON object \\\n",
    "        with your interpretation of quantity inter-dependencies in a parent-children relation.\n",
    "        For example: {example}\n",
    "        Answer between the delimitator ``````, with only a JSON string and no other conversations.\n",
    "        ---\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "prompt4_1 = prompt_4(question, response3_1)\n",
    "prompt4_2 = prompt_4(question, response3_2)\n",
    "print(\"ROUND3 - MAKE TABLE\")\n",
    "response4_1 = get_completion(prompt4_1,model=\"gpt-3.5-turbo\")\n",
    "print(\"GPT3.5: \" + \"\\n\" + response4_1)\n",
    "response4_2 = get_completion(prompt4_2,model=\"gpt-4-0314\")\n",
    "print(\"GPT4: \" + \"\\n\" + response4_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response4_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m         new_s \u001b[39m=\u001b[39m new_s\u001b[39m.\u001b[39mreplace(i, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(new_s\u001b[39m.\u001b[39msplit())\n\u001b[1;32m----> 8\u001b[0m json_1 \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(remove_delimiters(\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m, response4_1)) \n\u001b[0;32m      9\u001b[0m json_2 \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(remove_delimiters(\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m, response4_2)) \n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(json_1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response4_1' is not defined"
     ]
    }
   ],
   "source": [
    "# extract JSON from a string\n",
    "def remove_delimiters(delimiters, s):\n",
    "    new_s = s\n",
    "    for i in delimiters: #replace each delimiter in turn with a space\n",
    "        new_s = new_s.replace(i, ' ')\n",
    "    return ' '.join(new_s.split())\n",
    "\n",
    "json_1 = json.loads(remove_delimiters(\"`\", response4_1)) \n",
    "json_2 = json.loads(remove_delimiters(\"`\", response4_2)) \n",
    "print(json_1)\n",
    "print(json_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Step 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: clarification - check whether LLM 'understands' the input question.\n",
    "\n",
    "## reflection => what if we replace this step to \"capture user's intention\"?\n",
    "def clarify(query):\n",
    "    return f\"\"\"\\\n",
    "A curious user will ask you an estimation question and test if you understand what the question means.\n",
    "You will respond by paraphrase the question into a statement.\n",
    "For example, if the user ask: what is the number of piano tuners in Chicago?\n",
    "Your response will be: You'd like to estimate the number of piano tuners in the city of Chicago, the USA.\n",
    "If you don't understand the question, respond: \"Sorry I don't fully understand the question. Could you please clarify?\"\n",
    "If the question is not relevant to quantity estimation, respond with \"Sorry, but I cannot answer that question.\"\n",
    "Respond with maximum 3 sentences.\n",
    "\n",
    "User: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: information extraction - if user accept model interpretation start extract information\n",
    "def extract(original_question, model_response):\n",
    "    return f\"\"\"\\\n",
    "A curious user asked an estimation question and you responded with your interpretation.\n",
    "The user asked: {original_question}\n",
    "You responded: {model_response}\n",
    "Now will extract information by identifying the final quantify of interest,\n",
    "and what factors this quantity is dependent on. Do not give numerical estimation yet.\n",
    "Respond with maximum 5 sentences.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# step 3: \n",
    "table = f\"\"\"\\\n",
    "| Quantity | value | unit |\n",
    "|-----|-----|-----|\n",
    "|[final quantity of interest] |[Leave this bank]|[unit based on the question]|\n",
    "|[relevant quantity A] |[Leave this bank] |[unit based on the question] |\n",
    "|...|...|...|\n",
    "\"\"\"\n",
    "\n",
    "def tabulate(original_question, model_response_2, example=table):\n",
    "    return f\"\"\"\\\n",
    "A curious user asked an estimation question and you responded with your interpretation. \n",
    "The user asked: {original_question}\n",
    "You identified final quantity of interest, and what factors or other quantities it depends on.\n",
    "Now you will translate all relevant quantities from your response into a Markdown table: \n",
    "{model_response_2}.\n",
    "For example:\n",
    "{example}\n",
    "\n",
    "Answer with only markdown table and no conversations.\n",
    "---\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: data transformation (convert table to json - can we hard-code this?)\n",
    "example_json = {\n",
    "  \"name\": \"quantity of interest\",\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"sub-quantity 1\",\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"sub-quantity 1.1\",\n",
    "          \"children\": []\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sub-quantity 2\",\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"sub-quantity 2.1\",\n",
    "          \"children\": []\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "zero_shot_example = json.dumps(example_json)\n",
    "def convertToJSON(original_question, model_response_table, example=zero_shot_example):\n",
    "    return f\"\"\"\\\n",
    "A curious user asked an estimation question and you responded with your interpretation.\n",
    "The user asked: {original_question}\n",
    "You identified final quantity of interest and what factors or other quantities it depends on.\n",
    "Then you extracted all relevant quantities into a Markdown table:\n",
    "{model_response_table}.\n",
    "Now you will transform the information from the table into a nested JSON object\n",
    "with your interpretation of quantity inter-dependencies in a parent-children relation.\n",
    "For example: \n",
    "{example}\n",
    "Answer between the delimitator ``````, with only a JSON string and no other conversations.\n",
    "---\n",
    "Answer:```[your answer]```\n",
    "\"\"\"\n",
    "\n",
    "# output validation\n",
    "# extract JSON from a string\n",
    "def remove_delimiters(delimiters, s):\n",
    "    new_s = s\n",
    "    for i in delimiters: #replace each delimiter in turn with a space\n",
    "        new_s = new_s.replace(i, ' ')\n",
    "    return ' '.join(new_s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "- User query: How many characters are there in this sentence??\n",
      ">>>>\n",
      "You're asking for an estimate of the number of characters in the sentence you just wrote. It appears to be 38 characters long, including spaces and punctuation.\n",
      "- Extracted information: \n",
      ">>>>\n",
      "The final quantity of interest is the number of characters in the sentence. This quantity is dependent on the length of the sentence, including spaces and punctuation. The sentence may also contain special characters or emojis, which would affect the final count. The font and size of the text may also impact the character count. Finally, the language used in the sentence may have different character counts for certain letters or symbols.\n",
      "- Format to Table:\n",
      ">>>>\n",
      "| Quantity | value | unit |\n",
      "|-----|-----|-----|\n",
      "|Final quantity of interest |Number of characters |Count |\n",
      "|Length of sentence |Varies |Count |\n",
      "|Special characters/emojis |Varies |Count |\n",
      "|Font and size of text |Varies |N/A |\n",
      "|Language used |Varies |N/A |\n",
      "- Format to JSON:\n",
      ">>>>\n",
      "{\n",
      "  \"name\": \"Number of characters\",\n",
      "  \"children\": [\n",
      "    {\n",
      "      \"name\": \"Length of sentence\",\n",
      "      \"children\": []\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Special characters/emojis\",\n",
      "      \"children\": []\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Font and size of text\",\n",
      "      \"children\": []\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Language used\",\n",
      "      \"children\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Validated JSON:\n",
      "{'name': 'Number of characters', 'children': [{'name': 'Length of sentence', 'children': []}, {'name': 'Special characters/emojis', 'children': []}, {'name': 'Font and size of text', 'children': []}, {'name': 'Language used', 'children': []}]}\n",
      "gpt-4-0314\n",
      "- User query: How many characters are there in this sentence??\n",
      ">>>>\n",
      "You're asking for an estimate of the number of characters in the sentence you just wrote. It appears to be 38 characters long, including spaces and punctuation.\n",
      "- Extracted information: \n",
      ">>>>\n",
      "The final quantity of interest is the number of characters in the sentence. This quantity is dependent on the length of the sentence, including spaces and punctuation. The sentence may contain different types of characters, such as letters, numbers, and symbols. The font and size of the text may also affect the number of characters. The language used in the sentence may also impact the number of characters.\n",
      "- Format to Table:\n",
      ">>>>\n",
      "| Quantity | value | unit |\n",
      "|-----|-----|-----|\n",
      "|Final quantity of interest |Number of characters |Count |\n",
      "|Length of sentence |Varies |Count |\n",
      "|Types of characters |Letters, numbers, symbols |N/A |\n",
      "|Font and size |Varies |N/A |\n",
      "|Language |Varies |N/A |\n",
      "- Format to JSON:\n",
      ">>>>\n",
      "{\"name\": \"Number of characters\", \"children\": [{\"name\": \"Length of sentence\", \"children\": []}, {\"name\": \"Types of characters\", \"children\": []}, {\"name\": \"Font and size\", \"children\": []}, {\"name\": \"Language\", \"children\": []}]}\n",
      "Validated JSON:\n",
      "{'name': 'Number of characters', 'children': [{'name': 'Length of sentence', 'children': []}, {'name': 'Types of characters', 'children': []}, {'name': 'Font and size', 'children': []}, {'name': 'Language', 'children': []}]}\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Inference\n",
    "question = \"How many characters are there in this sentence??\"\n",
    "\n",
    "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
    "    print(model)\n",
    "\n",
    "    print(\"- User query: \" + question)\n",
    "    response1 = get_completion(clarify(question))\n",
    "\n",
    "    print(\"- Extracted information: \")\n",
    "    response2 = get_completion(extract(question, response1))\n",
    "\n",
    "    print(\"- Format to Table:\")\n",
    "    response3 = get_completion(tabulate(question, response2))\n",
    "\n",
    "    print(\"- Format to JSON:\")\n",
    "    response4 = get_completion(convertToJSON(question, response3))\n",
    "    # note sometimes the response will return with ``````, so we can use remove_delimiters function to filter out,\n",
    "    # or use a \"json extractor\" function to extract the first json object from a returned message (risk of prompt injection)\n",
    "    try:\n",
    "        json_1 = json.loads(remove_delimiters(\"`\", response4))\n",
    "        print(\"Validated JSON:\")\n",
    "        print(json_1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT4\n",
    "print(\"- User query: \" + question)\n",
    "response1 = get_completion(clarify(question), model=\"gpt-4\")\n",
    "\n",
    "print(\"- Extracted information: \")\n",
    "response2 = get_completion(extract(question, response1), model=\"gpt-4\")\n",
    "\n",
    "print(\"- Format to Table:\")\n",
    "response3 = get_completion(tabulate(question, response2), model=\"gpt-4\")\n",
    "\n",
    "print(\"- Format to JSON:\")\n",
    "response4 = get_completion(convertToJSON(question, response3), model=\"gpt-4\")\n",
    "# note sometimes the response will return with ``````, so we can use remove_delimiters function to filter out,\n",
    "# or use a \"json extractor\" function to extract the first json object from a returned message (risk of prompt injection)\n",
    "json_1 = json.loads(remove_delimiters(\"`\", response4))\n",
    "print(json_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to count the number of tokens for each query\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(tabulate(question, response2), \"cl100k_base\")\n",
    "\n",
    "# count the number of tokens from the last response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "- Check assumption: What's the size of the green door on the Moon?\n",
      ">>>>\n",
      "As an AI language model, I cannot provide assumptions made by the user without any context or information about the statement. Please provide more details or context for me to assist you better.\n",
      "gpt-4-0314\n",
      "- Check assumption: What's the size of the green door on the Moon?\n",
      ">>>>\n",
      "- The user assumes that the statement is clear and understandable.\n",
      "- The user assumes that the statement is based on accurate information or personal beliefs.\n",
      "- The user assumes that the statement is relevant to the context or topic being discussed.\n",
      "- The user assumes that the statement will be received and interpreted as intended.\n",
      "- The user assumes that the statement will provoke thought or elicit a response from the audience.\n",
      "- The user assumes that the statement is appropriate for the audience or platform it is being shared on.\n"
     ]
    }
   ],
   "source": [
    "# check assumption\n",
    "\n",
    "\n",
    "def checkAssumption(statement):\n",
    "    template = \"\"\"Here is a statement:\n",
    "{statement}\n",
    "Make a bullet point list of the assumptions the user made when producing the given statement.\\n\\n\"\"\"\n",
    "    return template\n",
    "\n",
    "statement = \"What's the size of the green door on the Moon?\"\n",
    "\n",
    "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
    "    print(model)\n",
    "    print(\"- Check assumption: \" + statement)\n",
    "    response = get_completion(checkAssumption(statement), model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = f\"\"\"\\\n",
    "        A curious user asked an estimation question and you responded with your interpretation.\n",
    "        The user asked: {question}\n",
    "        You identified final quantity of interest and what factors or other quantities it depends on.\n",
    "        Then you extract all relevant quantities into a Markdown table:\n",
    "        {response2_1}.\n",
    "        Now you will transform the information from the table into a nested JSON object\n",
    "        with your interpretation of quantity inter-dependencies in a parent-children relation.\n",
    "        For example: {example}\n",
    "        Answer between the delimitator ``````, with only a JSON string and no other conversations.\n",
    "        ---\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "print(table)\n",
    "\n",
    "table2 = f\"\"\"\\\n",
    "| Quantity | value | unit |\n",
    "|-----|-----|-----|\n",
    "|[final quantity of interest] |[Leave this bank]|[unit based on the question]|\n",
    "|[relevant quantity A] |[Leave this bank] |[unit based on the question] |\n",
    "|...|...|...|\n",
    "\"\"\"\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "- Check assumption: What's the weight of Lithum in obsolete batteries from electric cars produced in 2020?\n",
      ">>>>\n",
      "Key variables/factors:\n",
      "- Number of electric cars produced in 2020\n",
      "- Average weight of lithium in a single battery\n",
      "- Average number of batteries per electric car\n",
      "- Percentage of obsolete batteries from electric cars produced in 2020\n",
      "- Efficiency of the recycling process for lithium in batteries\n",
      "\n",
      "Mathematical model:\n",
      "Weight of Lithium in Obsolete Batteries = (Number of Electric Cars Produced in 2020) x (Average Weight of Lithium in a Single Battery) x (Average Number of Batteries per Electric Car) x (Percentage of Obsolete Batteries from Electric Cars Produced in 2020) x (Efficiency of the Recycling Process for Lithium in Batteries)\n",
      "gpt-4-0314\n",
      "- Check assumption: What's the weight of Lithum in obsolete batteries from electric cars produced in 2020?\n",
      ">>>>\n",
      "Key variables or factors:\n",
      "\n",
      "- Number of electric cars produced in 2020\n",
      "- Percentage of electric cars using lithium-ion batteries\n",
      "- Average weight of a lithium-ion battery in an electric car\n",
      "- Lithium content percentage in a lithium-ion battery\n",
      "- Percentage of batteries becoming obsolete in 2020\n",
      "\n",
      "Mathematical model:\n",
      "\n",
      "Let:\n",
      "N = Number of electric cars produced in 2020\n",
      "P = Percentage of electric cars using lithium-ion batteries\n",
      "W = Average weight of a lithium-ion battery in an electric car\n",
      "L = Lithium content percentage in a lithium-ion battery\n",
      "O = Percentage of batteries becoming obsolete in 2020\n",
      "\n",
      "Total weight of lithium in obsolete batteries from electric cars produced in 2020 = N * P * W * L * O\n"
     ]
    }
   ],
   "source": [
    "# check assumption\n",
    "\n",
    "\n",
    "def determineFactor(user_query):\n",
    "    template = f\"\"\"\\\n",
    "Please assist me in identifying the key variables or factors that would influence the estimation of the following question: {user_query}. \\\n",
    "Please list them in bullet points.\\n\n",
    "Then try to form a mathematical model.\"\"\"\n",
    "    return template\n",
    "\n",
    "user_query = \"What's the weight of Lithum in obsolete batteries from electric cars produced in 2020?\"\n",
    "\n",
    "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
    "    print(model)\n",
    "    print(\"- Check assumption: \" + user_query)\n",
    "    response = get_completion(determineFactor(user_query), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      ">>>>\n",
      "First, we can use the second equation to solve for b:\n",
      "-b - 2 = 0\n",
      "-b = 2\n",
      "b = -2\n",
      "\n",
      "Next, we can use the third equation to solve for c:\n",
      "c + 1 = 0\n",
      "c = -1\n",
      "\n",
      "Substituting these values into the first equation, we get:\n",
      "a - 6 = 0\n",
      "a = 6\n",
      "\n",
      "Therefore, the solution is:\n",
      "{'a': 6, 'b': -2, 'c': -1}\n",
      "gpt-4-0314\n",
      ">>>>\n",
      "First, let's solve the second and third equations for b and c, respectively.\n",
      "\n",
      "Equation 2: -b - 2 = 0\n",
      "Add 2 to both sides:\n",
      "-b = 2\n",
      "Multiply both sides by -1:\n",
      "b = -2\n",
      "\n",
      "Equation 3: c + 1 = 0\n",
      "Subtract 1 from both sides:\n",
      "c = -1\n",
      "\n",
      "Now, let's substitute the values of b and c into the first equation.\n",
      "\n",
      "Equation 1: a + b - 3c + 1 = 0\n",
      "Substitute b = -2 and c = -1:\n",
      "a + (-2) - 3(-1) + 1 = 0\n",
      "\n",
      "Simplify:\n",
      "a - 2 + 3 + 1 = 0\n",
      "a + 2 = 0\n",
      "\n",
      "Subtract 2 from both sides:\n",
      "a = -2\n",
      "\n",
      "Now, we can write the result in a dictionary:\n",
      "\n",
      "Result: {'a': -2, 'b': -2, 'c': -1}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\\\n",
    "Solve the following equation, show your working steps, and then finally return the result in a dictionary where key is the variable, and the value is the value after the flag token.\n",
    "Question: [a + b - 3c + 1 = 0; -b - 2 = 0, c + 1 = 0]\n",
    "Solution:\n",
    "Result:\n",
    "\"\"\"\n",
    "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
    "    print(model)\n",
    "    response = get_completion(prompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      ">>>>\n",
      "To estimate the number of ice creams sold in Brighton, UK each year, we can follow these steps:\n",
      "\n",
      "1. Find the population of Brighton, UK. According to the latest census data, the population of Brighton is around 290,000.\n",
      "\n",
      "2. Estimate the number of ice creams consumed per person per year. This can vary widely depending on factors such as age, income, and weather. However, a reasonable estimate could be around 20-30 ice creams per person per year.\n",
      "\n",
      "3. Multiply the population by the estimated number of ice creams consumed per person per year. Using the midpoint of the estimate (25 ice creams per person per year), we get:\n",
      "\n",
      "290,000 x 25 = 7,250,000\n",
      "\n",
      "So, based on these estimates, we can say that around 7-8 million ice creams are sold in Brighton, UK each year. This can be expressed in order of magnitude as 10^7.\n",
      "gpt-4-0314\n",
      ">>>>\n",
      "To estimate the number of ice creams sold in Brighton, UK each year, we can follow these steps:\n",
      "\n",
      "1. Estimate the population of Brighton: According to recent data, the population of Brighton is approximately 290,000 people.\n",
      "\n",
      "2. Estimate the number of tourists visiting Brighton each year: Brighton is a popular tourist destination, with around 11 million visitors per year.\n",
      "\n",
      "3. Estimate the percentage of people who buy ice cream: Let's assume that 50% of both residents and tourists buy ice cream at least once a year. This is a rough estimate, as it can vary depending on factors like weather, personal preferences, and availability of ice cream shops.\n",
      "\n",
      "4. Calculate the total number of ice cream buyers: Multiply the population and the percentage of residents who buy ice cream (290,000 * 0.5 = 145,000) and do the same for tourists (11,000,000 * 0.5 = 5,500,000).\n",
      "\n",
      "5. Add the two numbers together: 145,000 (residents) + 5,500,000 (tourists) = 5,645,000 ice creams sold per year.\n",
      "\n",
      "Now, we need to express this number in the form of 10^(n). To do this, we can take the logarithm base 10 of the number:\n",
      "\n",
      "log10(5,645,000) â‰ˆ 6.75\n",
      "\n",
      "So, our estimate for the number of ice creams sold in Brighton, UK each year is 10^(6.75).\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\\\n",
    "A curious learner is asking you a question:\n",
    "How many ice creams are sold in Brighton, UK each year?\n",
    "You know the answer is in the order of magnitude, and you will now tell the learner, step by step, how they can work out the answer themselves. End your response with 10^(n), where n is your estimated answer.\n",
    "\"\"\"\n",
    "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
    "    print(model)\n",
    "    response = get_completion(prompt, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Wolfram Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updaintg safe_quantity_dict:lenth=[length]^\n",
      "Quantity density found no result.\n",
      "Updaintg safe_quantity_dict:mass=[mass]^\n",
      "Updaintg safe_quantity_dict:time=[time]^\n",
      "Updaintg safe_quantity_dict:power=[length]^2 [mass]^ [time]^(-3)\n",
      "Updaintg safe_quantity_dict:energy=[length]^2 [mass]^ [time]^(-2)\n",
      "Updaintg safe_quantity_dict:force=[length]^ [mass]^ [time]^(-2)\n",
      "Updaintg safe_quantity_dict:pressure=[length]^(-1) [mass]^ [time]^(-2)\n",
      "Updaintg safe_quantity_dict:velocity=[length]^ [time]^(-1)\n",
      "Updaintg safe_quantity_dict:acceleration=[length]^ [time]^(-2)\n",
      "Updaintg safe_quantity_dict:area=[length]^2\n",
      "Updaintg safe_quantity_dict:volume=[length]^3\n",
      "Updaintg safe_quantity_dict:temperature=[temperature]^\n",
      "Quantity charge found no result.\n",
      "Updaintg safe_quantity_dict:voltage=[electric current]^(-1) [length]^2 [mass]^ [time]^(-3)\n",
      "Quantity current found no result.\n",
      "Quantity resistance found no result.\n",
      "Quantity magnetic field found no result.\n",
      "Quantity inductance found no result.\n",
      "Updaintg safe_quantity_dict:frequency=[time]^(-1)\n",
      "Quantity angle found no result.\n",
      "Updaintg safe_quantity_dict:angular velocity=[angle]^ [time]^(-1)\n",
      "Updaintg safe_quantity_dict:angular acceleration=[angle]^ [time]^(-2)\n",
      "Updaintg safe_quantity_dict:momentum=[length]^ [mass]^ [time]^(-1)\n",
      "Updaintg safe_quantity_dict:angular momentum=[length]^2 [mass]^ [time]^(-1)\n",
      "Updaintg safe_quantity_dict:torque=[length]^2 [mass]^ [time]^(-2)\n",
      "Updaintg safe_quantity_dict:moment of inertia=[length]^2 [mass]^\n",
      "Updaintg safe_quantity_dict:impulse=[length]^ [mass]^ [time]^(-1)\n",
      "Updaintg safe_quantity_dict:work=[length]^2 [mass]^ [time]^(-2)\n",
      "Updaintg safe_quantity_dict:power=[length]^2 [mass]^ [time]^(-3)\n",
      "Updaintg safe_quantity_dict:energy=[length]^2 [mass]^ [time]^(-2)\n",
      "Quantity density found no result.\n",
      "Quantity specific heat found no result.\n",
      "Updaintg safe_quantity_dict:heat capacity=[length]^2 [mass]^ [temperature]^(-1) [time]^(-2)\n",
      "Updaintg safe_quantity_dict:thermal conductivity=[length]^ [mass]^ [temperature]^(-1) [time]^(-3)\n",
      "Quantity thermal expansion found no result.\n",
      "Quantity viscosity found no result.\n",
      "Updaintg safe_quantity_dict:surface tension=[mass]^ [time]^(-2)\n",
      "Quantity electric field found no result.\n",
      "Updaintg safe_quantity_dict:electric flux=[electric current]^(-1) [length]^3 [mass]^ [time]^(-3)\n",
      "Updaintg safe_quantity_dict:electric potential=[electric current]^(-1) [length]^2 [mass]^ [time]^(-3)\n",
      "Updaintg safe_quantity_dict:electric resistance=[electric current]^(-2) [length]^2 [mass]^ [time]^(-3)\n",
      "Updaintg safe_quantity_dict:electric resistivity=[electric current]^(-2) [length]^3 [mass]^ [time]^(-3)\n",
      "Updaintg safe_quantity_dict:electric conductance=[electric current]^2 [length]^(-2) [mass]^(-1) [time]^3\n",
      "Updaintg safe_quantity_dict:electric conductivity=[electric current]^2 [length]^(-3) [mass]^(-1) [time]^3\n",
      "Updaintg safe_quantity_dict:electric capacitance=[electric current]^2 [length]^(-2) [mass]^(-1) [time]^4\n",
      "Quantity electric inductance found no result.\n"
     ]
    }
   ],
   "source": [
    "def query_wolfram_alpha(query):\n",
    "    app_id = os.getenv('chatWolf')\n",
    "    client = wolframalpha.Client(app_id)\n",
    "    res = client.query(query)\n",
    "\n",
    "    try:\n",
    "        return next(res.results).text\n",
    "    except StopIteration:\n",
    "        return \"No results found.\"\n",
    "\n",
    "# Example usage:\n",
    "test_quantity_list = [\"lenth\", \"density\", \"mass\", \"time\", \"power\",\n",
    "                       \"energy\", \"force\", \"pressure\", \"velocity\", \"acceleration\",\n",
    "                         \"area\", \"volume\", \"temperature\", \"charge\", \"voltage\", \"current\",\n",
    "                           \"resistance\", \"magnetic field\", \"inductance\", \"frequency\", \"angle\",\n",
    "                             \"angular velocity\", \"angular acceleration\", \"momentum\", \"angular momentum\",\n",
    "                               \"torque\", \"moment of inertia\", \"impulse\", \"work\", \"power\", \"energy\", \"density\",\n",
    "                                 \"specific heat\", \"heat capacity\", \"thermal conductivity\", \"thermal expansion\",\n",
    "                                   \"viscosity\", \"surface tension\", \"electric field\", \"electric flux\", \"electric potential\",\n",
    "                                     \"electric resistance\", \"electric resistivity\", \"electric conductance\", \"electric conductivity\",\n",
    "                                       \"electric capacitance\", \"electric inductance\"] \n",
    "                                    \n",
    "safe_quantity_dict = {}\n",
    "unsafe_cases = []\n",
    "# filter out unsafe cases\n",
    "for i in range(len(test_quantity_list)):\n",
    "    quantity = test_quantity_list[i]\n",
    "    query = f\"\"\"dimensions of {quantity}\"\"\"\n",
    "    result = query_wolfram_alpha(query)\n",
    "    if (result == \"No results found.\"):\n",
    "        print(f\"Quantity {quantity} found no result.\")\n",
    "        # check if quanity is in unsafe_cases\n",
    "        if (quantity not in unsafe_cases):\n",
    "            unsafe_cases.append(quantity)\n",
    "    # update safe_quantity_dict, key is quantity, value is result\n",
    "    else:\n",
    "        print(\"Updaintg safe_quantity_dict:\" + quantity + \"=\" + result )\n",
    "        safe_quantity_dict[quantity] = result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['density', 'charge', 'current', 'resistance', 'magnetic field', 'inductance', 'angle', 'specific heat', 'thermal expansion', 'viscosity', 'electric field', 'electric inductance']\n",
      "{'lenth': '[length]^', 'mass': '[mass]^', 'time': '[time]^', 'power': '[length]^2 [mass]^ [time]^(-3)', 'energy': '[length]^2 [mass]^ [time]^(-2)', 'force': '[length]^ [mass]^ [time]^(-2)', 'pressure': '[length]^(-1) [mass]^ [time]^(-2)', 'velocity': '[length]^ [time]^(-1)', 'acceleration': '[length]^ [time]^(-2)', 'area': '[length]^2', 'volume': '[length]^3', 'temperature': '[temperature]^', 'voltage': '[electric current]^(-1) [length]^2 [mass]^ [time]^(-3)', 'frequency': '[time]^(-1)', 'angular velocity': '[angle]^ [time]^(-1)', 'angular acceleration': '[angle]^ [time]^(-2)', 'momentum': '[length]^ [mass]^ [time]^(-1)', 'angular momentum': '[length]^2 [mass]^ [time]^(-1)', 'torque': '[length]^2 [mass]^ [time]^(-2)', 'moment of inertia': '[length]^2 [mass]^', 'impulse': '[length]^ [mass]^ [time]^(-1)', 'work': '[length]^2 [mass]^ [time]^(-2)', 'heat capacity': '[length]^2 [mass]^ [temperature]^(-1) [time]^(-2)', 'thermal conductivity': '[length]^ [mass]^ [temperature]^(-1) [time]^(-3)', 'surface tension': '[mass]^ [time]^(-2)', 'electric flux': '[electric current]^(-1) [length]^3 [mass]^ [time]^(-3)', 'electric potential': '[electric current]^(-1) [length]^2 [mass]^ [time]^(-3)', 'electric resistance': '[electric current]^(-2) [length]^2 [mass]^ [time]^(-3)', 'electric resistivity': '[electric current]^(-2) [length]^3 [mass]^ [time]^(-3)', 'electric conductance': '[electric current]^2 [length]^(-2) [mass]^(-1) [time]^3', 'electric conductivity': '[electric current]^2 [length]^(-3) [mass]^(-1) [time]^3', 'electric capacitance': '[electric current]^2 [length]^(-2) [mass]^(-1) [time]^4'}\n"
     ]
    }
   ],
   "source": [
    "print(unsafe_cases)\n",
    "print(safe_quantity_dict)\n",
    "\n",
    "# save safe_quantity_dict to json file\n",
    "with open('safe_quantity_dict.json', 'w') as fp:\n",
    "    json.dump(safe_quantity_dict, fp, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
