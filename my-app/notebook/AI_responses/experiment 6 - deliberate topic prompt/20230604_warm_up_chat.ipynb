{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 0605 - can module survive high temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "%pip install python-dotenv\n",
    "%pip install openai\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv # load keys from .env file\n",
    "import openai # use OpenAI API\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tiktoken\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4-0314\")\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-4-0314\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the json object at the end using regular expressions\n",
    "def get_json(text):\n",
    "    '''extract the json object from the text using regular expressions'''\n",
    "    match = re.search(r'{.*}', text)\n",
    "    if match:\n",
    "        json_string = match.group()\n",
    "        # json_object = json.loads(json_string)\n",
    "        return json_string\n",
    "\n",
    "def get_python(text):\n",
    "    '''extract the python code from the text using regular expressions'''\n",
    "    pattern = re.compile(r'\"\"\"python(.*?)\"\"\"', re.DOTALL)\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        code = match.group(1)\n",
    "        return code.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    '''Query OpenAI API to get a completion from a naive prompt'''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=500, # this is the maximum number of returned tokens\n",
    "    )\n",
    "    print(\">>>>RESPONSE>>>>\")\n",
    "    print(str(response.choices[0].message[\"content\"]))\n",
    "    print(\"<<<<END<<<<\")\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=1.5,max_tokens=2000):\n",
    "    '''ChatML format to query OpenAI API to get a completion from a list of messages'''\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# write function to combine two messages\n",
    "# turn the above process into a function\n",
    "def get_answer_step_by_step(question, model=\"gpt-4\", get_completion_from_messages=get_completion_from_messages):\n",
    "    '''Query OpenAI API to answer a question step-by-step'''\n",
    "    system_message = \"\"\"A user will give you a problem. Follow these steps.\n",
    "\n",
    "Step 1: Reason step-by-step to clearly define the problem and what additional information is desired.\n",
    "\n",
    "Step 2: Reason step-by-step a methodology that can be used to answer the problem.\n",
    "\n",
    "Step 3: Calculate the results based on step 2.\n",
    "\n",
    "Step 4: Stating assumptions used in the methodology and the results.\n",
    "\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": question}]\n",
    "    result = get_completion_from_messages(messages, model=model) # returns a string\n",
    "    return result\n",
    "\n",
    "def text_to_json(message, model=\"gpt-3.5\"):\n",
    "    '''Query OpenAI API to convert text to a parsable JSON'''\n",
    "    JSON_Parser_system_message = '''\\\n",
    "A user will give you a text contain 4 steps. Your task is to convert the text to make it more readable. Each step will be the key, and the details will be the value.\n",
    "Your output will be a JSON object in deliminator (\"\"\") looks like this: \n",
    "JSON={\"Step 1\": \"details in step 1\", \"Step 2\": \"details in step 2\", \"Step 3\": \"details in step 3\", \"Step 4\": \"details in step 4\"}\n",
    "Here's an example:\n",
    "INPUT:\n",
    "======\n",
    "Step 1: \n",
    "To answer this problem, we need to know the following information:\n",
    "1. How much power is required to jump start a car?\n",
    "2. How much power can be generated by one lemon battery?\n",
    "3. How many lemon batteries are required to generate the power needed to jump start a car?\n",
    "\n",
    "Step 2:\n",
    "To answer this problem, we can follow these steps:\n",
    "1. Determine the power required to jump start a car.\n",
    "2. Determine the power generated by one lemon battery.\n",
    "3. Calculate the number of lemon batteries required to generate the power needed to jump start a car.\n",
    "\n",
    "Step 3:\n",
    "1. According to AAA, the power required to jump start a car is around 300-400 amps for 5-10 seconds. Let's assume that we need 350 amps for 7 seconds to jump start a car.\n",
    "2. One lemon battery can generate around 0.7 volts and 0.0003 amps of current. Using Ohm's law, we can calculate the power generated by one lemon battery as follows: Power = Voltage x Current = 0.7 x 0.0003 = 0.00021 watts or 0.21 milliwatts.\n",
    "3. To generate 350 amps for 7 seconds, we need a total power of 350 x 7 = 2450 watts or 2.45 kilowatts. To generate this much power using lemon batteries, we need 2.45 / 0.00021 = 11,666.67 lemon batteries.\n",
    "\n",
    "Step 4:\n",
    "Assumptions:\n",
    "1. The power required to jump start a car is assumed to be 350 amps for 7 seconds.\n",
    "2. The power generated by one lemon battery is assumed to be 0.21 milliwatts.\n",
    "3. The lemon batteries are assumed to be connected in series to generate the required voltage and current.\n",
    "\n",
    "======\n",
    "OUTPUT:\"\"\"\n",
    "JSON={\"problem definition\": \"If a lemon can be made into a battery, how many lemons can jump start a car?\", \"methodology\": \"To answer this problem, we need to know the power required to jump start a car, the power generated by one lemon battery, and the number of lemon batteries required to generate the power needed to jump start a car. We can calculate the number of lemon batteries required by dividing the required power by the power generated by one lemon battery.\", \"results\": \"To jump start a car, we need 11,666.67 lemon batteries.\", \"assumptions\": \"The power required to jump start a car is assumed to be 350 amps for 7 seconds. The power generated by one lemon battery is assumed to be 0.21 milliwatts. The lemon batteries are assumed to be connected in series to generate the required voltage and current.\"}\n",
    "\"\"\"\n",
    "'''\n",
    "    message_wrapper = f'''Convert message into JSON object.\\nInput:\\n==={message}\\n===\\n OUTPUT:\\n \"\"\"[JSON=]\"\"\"'''\n",
    "    get_json_messages = [{\"role\": \"system\", \"content\": JSON_Parser_system_message},\n",
    "    {\"role\": \"user\", \"content\": message_wrapper}]\n",
    "\n",
    "    JSON_result = get_completion_from_messages(get_json_messages) # returns a string\n",
    "    return JSON_result\n",
    "\n",
    "def text_to_python(message, model=\"gpt-4\"):\n",
    "    '''Query OpenAI API to convert text to python'''\n",
    "    Python_generator_system_message = '''A user will give you a text contain 4 steps. Your task is to edit the text to make it more readable in the following format.\n",
    "Output a JSON object after deliminator \"\"\" in the following structure: JSON={\"Step 1\": \"details in step 1\", \"Step 2\": \"details in step 2\", \"Step 3\": \"details in step 3\", \"Step 4\": \"details in step 4\"}\n",
    "One Example is as follows:\n",
    "INPUT TEXT:\n",
    "Step 1: \n",
    "To answer this problem, we need to know the following information:\n",
    "1. How much power is required to jump start a car?\n",
    "2. How much power can be generated by one lemon battery?\n",
    "3. How many lemon batteries are required to generate the power needed to jump start a car?\n",
    "\n",
    "Step 2:\n",
    "To answer this problem, we can follow these steps:\n",
    "1. Determine the power required to jump start a car.\n",
    "2. Determine the power generated by one lemon battery.\n",
    "3. Calculate the number of lemon batteries required to generate the power needed to jump start a car.\n",
    "\n",
    "Step 3:\n",
    "1. According to AAA, the power required to jump start a car is around 300-400 amps for 5-10 seconds. Let's assume that we need 350 amps for 7 seconds to jump start a car.\n",
    "2. One lemon battery can generate around 0.7 volts and 0.0003 amps of current. Using Ohm's law, we can calculate the power generated by one lemon battery as follows: Power = Voltage x Current = 0.7 x 0.0003 = 0.00021 watts or 0.21 milliwatts.\n",
    "3. To generate 350 amps for 7 seconds, we need a total power of 350 x 7 = 2450 watts or 2.45 kilowatts. To generate this much power using lemon batteries, we need 2.45 / 0.00021 = 11,666.67 lemon batteries.\n",
    "\n",
    "Step 4:\n",
    "Assumptions:\n",
    "1. The power required to jump start a car is assumed to be 350 amps for 7 seconds.\n",
    "2. The power generated by one lemon battery is assumed to be 0.21 milliwatts.\n",
    "3. The lemon batteries are assumed to be connected in series to generate the required voltage and current.\n",
    "\n",
    "OUTPUT PYTHON:\n",
    "\"\"\"python\n",
    "    # Step 1: Define the variables\n",
    "    power_car_jump_start_amps = 350    # Amps\n",
    "    time_car_jump_start_seconds = 7    # Seconds\n",
    "\n",
    "    lemon_battery_volts = 0.7          # Volts\n",
    "    lemon_battery_amps = 0.0003        # Amps\n",
    "\n",
    "    # Step 2: Calculate the power required to jump start a car and the power generated by one lemon battery\n",
    "\n",
    "    # Power = Voltage x Current\n",
    "    power_car_jump_start_watts = power_car_jump_start_amps * time_car_jump_start_seconds  # Watts\n",
    "    power_lemon_battery_watts = lemon_battery_volts * lemon_battery_amps  # Watts\n",
    "\n",
    "    # Step 3: Calculate the number of lemon batteries required to generate the power needed to jump start a car\n",
    "    number_of_lemon_batteries = power_car_jump_start_watts / power_lemon_battery_watts\n",
    "\n",
    "    # Step 4: Print the result\n",
    "    print(f\"You need approximately {number_of_lemon_batteries:.0f} lemon batteries to jump start a car.\")\n",
    "\"\"\"\n",
    "Your will only output the python code after \"\"\" in the following format:\n",
    "\"\"\"python\n",
    "# Step 1: Define the variables\n",
    "# ...\n",
    "# Last step: Print the result\n",
    "print(f\"...\")\n",
    "\"\"\"\n",
    "'''\n",
    "    get_python_messages = [{\"role\": \"system\", \"content\": Python_generator_system_message},\n",
    "    {\"role\": \"user\", \"content\": message}]\n",
    "    result = get_completion_from_messages(get_python_messages, model=model) # returns a string\n",
    "    return result\n",
    "# # Example of an OpenAI ChatCompletion request with stream=True\n",
    "# # https://platform.openai.com/docs/guides/chat\n",
    "\n",
    "# # a ChatCompletion request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and Run this code block to get an example output\n",
    "\n",
    "# prompt_example= f\"\"\"What is Fermi Estimation and how does it work?\"\"\"\n",
    "# response = get_completion(prompt_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many libraries are there in London?\n"
     ]
    }
   ],
   "source": [
    "# market sizing question generator\n",
    "# list of service providers\n",
    "service_providers = [\"piano tuners\", \"nurseries\", \"coffee shops\", \"restaurants\", \"libraries\", \"gyms\", \"hotels\", \"barber shops\", \"gas stations\", \"schools\"]\n",
    "\n",
    "# list of cities\n",
    "cities = [\"Chicago\", \"London\", \"Beijing\", \"New York\", \"Los Angeles\", \"Paris\", \"Berlin\", \"Tokyo\", \"Sydney\", \"Mumbai\"]\n",
    "\n",
    "# function to generate a question\n",
    "def generate_market_sizing_question():\n",
    "    service_provider = random.choice(service_providers)\n",
    "    city = random.choice(cities)\n",
    "    \n",
    "    question = f\"How many {service_provider} are there in {city}?\"\n",
    "    \n",
    "    return question\n",
    "\n",
    "# generate a question\n",
    "print(generate_market_sizing_question())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How many restaurants are there in Paris?', 'How many schools are there in Beijing?', 'How many hotels are there in Los Angeles?', 'How many coffee shops are there in Beijing?', 'How many nurseries are there in New York?', 'How many piano tuners are there in Tokyo?', 'How many gas stations are there in Sydney?', 'How many libraries are there in Tokyo?', 'How many hotels are there in Paris?', 'How many barber shops are there in Beijing?']\n"
     ]
    }
   ],
   "source": [
    "# warm up talk\n",
    "# turn the above process into a function\n",
    "you_are_fermi = \"Your name is Enrico Fermi. You are an expert physicist and mathematician. You are known for your ability to make good approximate calculations with little or no actual data. You are also known for your ability to solve problems using recursion.\"\n",
    "round_1_fermi = '''Hi Fermi, are you alright? I want to learn how to use your trick to do Fermi Estimation and improve my problem solving skills.'''\n",
    "round_1_response = '''Fermi estimation, also known as Fermi problems, is a way to make good approximate calculations with little or no actual data. The method is named after physicist Enrico Fermi as he was known for making good approximate calculations with little or no actual data.\n",
    "\n",
    "Fermi problems typically involve making reasonable assumptions about the problem, then performing simple calculations on those assumptions. Here's a step-by-step guide on how to apply this:\n",
    "\n",
    "1. Define the Problem Clearly: Start by identifying the problem you need to solve. Fermi problems often seem overwhelming at first glance due to the large numbers and broad scope involved. For example, \"How many piano tuners are there in New York City?\".\n",
    "\n",
    "2. Break Down the Problem: Next, you'll want to break down the problem into smaller, more manageable questions whose answers can be reasonably estimated. For the piano tuner question, you might ask:\n",
    "\n",
    "- How many people live in New York City?\n",
    "- How many households does that represent?\n",
    "- What fraction of households own a piano?\n",
    "- How often does a piano need to be tuned each year?\n",
    "- How many pianos can one tuner handle in a year?\n",
    "3. Make Estimates: Now, you'll need to make educated guesses to answer these smaller questions. You can often make a reasonable estimate based on known facts or basic assumptions about the world. For example, you might guess that one in every hundred households in New York City owns a piano, and that a piano needs to be tuned once a year.\n",
    "\n",
    "4. Perform Calculations: Once you have your estimates, you'll need to perform the necessary calculations to solve the problem. Remember, the goal here isn't to get a perfectly accurate answer, but rather to arrive at a ballpark figure that's within the right order of magnitude. So, if your calculations require complex operations, it's usually okay to round numbers or simplify to make the math easier.\n",
    "\n",
    "5. Review and Refine: Finally, you should always review your estimates and calculations to see if they make sense. If your answer seems unreasonable, you should go back and check your work, refine your estimates, or consider whether there are other aspects of the problem you haven't taken into account.\n",
    "\n",
    "Fermi estimation is an incredibly valuable tool for developing problem-solving skills and enhancing critical thinking. It encourages making reasonable assumptions, simplifying complex problems, and approximating â€“ all crucial skills in a wide range of fields.'''\n",
    "\n",
    "round_2_recursion = '''This is a very structured approach. I am interested in the second step, which is breaking down a problem into sub-problems. This is a recursive step. Can you explain the concept of recursion?'''\n",
    "round_2_response = '''Recursion is a fundamental concept in computer science, mathematics, and broader problem-solving that involves breaking a complex problem into smaller, manageable subproblems. The solution to the original problem is then constructed from the solutions of these subproblems.\n",
    "\n",
    "A recursive process has two key components: a base case and a recursive case. The base case is a condition that terminates the recursion, providing a direct solution without self-referencing. In contrast, the recursive case involves the function calling itself with smaller instances of the problem, leading towards the base case with each step.\n",
    "\n",
    "For example, a recursive function for calculating factorials in programming would include a base case (if n equals 0, return 1) and a recursive case (return n multiplied by the factorial of n-1). This function continually calls itself, moving closer to the base case each time, until it can start returning values.\n",
    "\n",
    "In Fermi estimation and similar problem-solving methods, recursion entails breaking the problem down into subproblems until they can be easily solved or estimated. This 'divide and conquer' approach helps navigate complex problems, with the solutions to subproblems combined to solve larger ones, eventually addressing the original problem.'''\n",
    "\n",
    "round_3_piano_tuners_explained = '''In the case of the classic piano tuner problem, would you explain how recursion is applied here?'''\n",
    "round_3_response = '''In the case of the piano tuner problem, the application of recursion isn't as pronounced as in a mathematical function or computer algorithm, but the concept of \"breaking down a problem into sub-problems until they are solvable\" is present, which aligns with the general idea of recursion.\n",
    "\n",
    "The original problem is: \"How many piano tuners are there in New York City?\" This is a difficult problem to answer directly due to the lack of readily available data, so we break it down into smaller problems. Here's how that might look:\n",
    "\n",
    "Estimate the population of New York City: This is a base problem we can solve with known data. As of my knowledge cutoff in 2021, the population of New York City was over 8 million people.\n",
    "\n",
    "Estimate the number of households in New York City: This is a smaller problem we can estimate with a reasonable assumption. For instance, we could assume an average of 2.5 people per household, which is roughly the average household size in the United States.\n",
    "\n",
    "Estimate the proportion of households with a piano: This is another smaller problem. We might not have an exact answer, but we can make a reasonable guess. Maybe 1 in every 50 households has a piano.\n",
    "\n",
    "Estimate the number of times a piano needs tuning per year: Yet another smaller problem. A piano might need to be tuned once or twice a year.\n",
    "\n",
    "Estimate the number of pianos a tuner can service in a year: This is the final smaller problem. A piano tuner might be able to tune 2 pianos a day, working 5 days a week, for 50 weeks a year.\n",
    "\n",
    "Each of these smaller problems contributes to the solution of the larger problem. Once we have estimates for each of these, we can combine them to find the total estimated number of piano tuners in New York City. This process is similar to recursion in that we're continually breaking down the problem into smaller, solvable problems until we're able to solve the original, more complex problem.'''\n",
    "\n",
    "counter = 0\n",
    "question_bank = []\n",
    "while counter < 10:\n",
    "    # check if the question is already in the bank\n",
    "    new_question = generate_market_sizing_question()\n",
    "    if new_question not in question_bank:\n",
    "            question_bank.append(new_question)\n",
    "            counter += 1\n",
    "print(question_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concept_recursion(chat_messages, model=\"gpt-4-0314\", get_completion_from_messages=get_completion_from_messages):\n",
    "    '''Query OpenAI API to answer a question using recursion'''\n",
    "    result = get_completion_from_messages(chat_messages, model=model) # returns a string\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question inference (with GPT4 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many restaurants are there in Paris?\n",
      "No. of token used: 1357\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:13:48 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '7f33758efd36bcf1d882b3ce5cad17a8', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d26f53b6d2f778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 0 in question bank<<<<\n",
      "<<<<Received Response to Question 0 <<<<\n",
      "<<<<Received Python Code to Question 0 <<<<\n",
      "How many schools are there in Beijing?\n",
      "No. of token used: 1357\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:14:46 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '5faf2e5874fa664198e7bd7ea1586a8c', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d26f6a6fa29778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 1 in question bank<<<<\n",
      "<<<<Received Response to Question 1 <<<<\n",
      "<<<<Received Python Code to Question 1 <<<<\n",
      "How many hotels are there in Los Angeles?\n",
      "No. of token used: 1358\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:15:26 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': 'b1becfaa56c10ff08895cf93e0a8a083', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d26f79c2a3f778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 2 in question bank<<<<\n",
      "<<<<Received Response to Question 2 <<<<\n",
      "<<<<Received Python Code to Question 2 <<<<\n",
      "How many coffee shops are there in Beijing?\n",
      "No. of token used: 1358\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:17:42 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '6143873912333aa14d310ff3e4569e65', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d26faee9bdc778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 3 in question bank<<<<\n",
      "<<<<Received Response to Question 3 <<<<\n",
      "<<<<Received Python Code to Question 3 <<<<\n",
      "How many nurseries are there in New York?\n",
      "No. of token used: 1359\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:19:46 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '32fa07193f40a81516ba8b33dd12e2ad', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d26fdf66c1f778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 4 in question bank<<<<\n",
      "<<<<Received Response to Question 4 <<<<\n",
      "<<<<Received Python Code to Question 4 <<<<\n",
      "How many piano tuners are there in Tokyo?\n",
      "No. of token used: 1359\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:21:04 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': 'e67607fa5bf0c60ad4bd90a76a1d4776', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d26ffdfaf6c778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 5 in question bank<<<<\n",
      "<<<<Received Response to Question 5 <<<<\n",
      "<<<<Received Python Code to Question 5 <<<<\n",
      "How many gas stations are there in Sydney?\n",
      "No. of token used: 1358\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:21:54 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '769132d950b67c357b0479c45669ffd9', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d270115ecf8778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 6 in question bank<<<<\n",
      "<<<<Received Response to Question 6 <<<<\n",
      "<<<<Received Python Code to Question 6 <<<<\n",
      "How many libraries are there in Tokyo?\n",
      "No. of token used: 1357\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:22:47 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '93eda4cbb4358ee7d74291b54dfd7d97', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d2702611e84778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 7 in question bank<<<<\n",
      "<<<<Received Response to Question 7 <<<<\n",
      "<<<<Received Python Code to Question 7 <<<<\n",
      "How many hotels are there in Paris?\n",
      "No. of token used: 1357\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:23:51 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '12e83355b06c9c2e4c1535ebc0818a3a', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d2703f32b05778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 8 in question bank<<<<\n",
      "<<<<Received Response to Question 8 <<<<\n",
      "<<<<Received Python Code to Question 8 <<<<\n",
      "How many barber shops are there in Beijing?\n",
      "No. of token used: 1358\n",
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Mon, 05 Jun 2023 08:25:30 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': 'ecfd31274599163c8274849bbfec902b', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d27065b3f8c778f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "<<<<Completed Question 9 in question bank<<<<\n",
      "<<<<Received Response to Question 9 <<<<\n",
      "<<<<Received Python Code to Question 9 <<<<\n",
      "['How many restaurants are there in Paris?', 'How many schools are there in Beijing?', 'How many hotels are there in Los Angeles?', 'How many coffee shops are there in Beijing?', 'How many nurseries are there in New York?', 'How many piano tuners are there in Tokyo?', 'How many gas stations are there in Sydney?', 'How many libraries are there in Tokyo?', 'How many hotels are there in Paris?', 'How many barber shops are there in Beijing?']\n",
      "Error\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate a question bank of 100 questions\n",
    "df = pd.DataFrame(columns=['question', 'answer_after_warm_up', 'answer_json', 'answer_python'])\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "for index, each_question in enumerate(question_bank):\n",
    "    question_prompt = f'''That's very helpful. Now let's consider another problem: {each_question} - how would you apply the concept of recursion to arrive at an answer?\n",
    "Can you use your estimation trick and recursively break the problem down until you can confidently provide a rough estimation?'''\n",
    "    warmed_messages = [{\"role\": \"system\", \"content\": you_are_fermi},\n",
    "            {\"role\": \"user\", \"content\": round_1_fermi},\n",
    "            {\"role\": \"system\", \"content\": round_1_response},\n",
    "            {\"role\": \"user\", \"content\": round_2_recursion},\n",
    "            {\"role\": \"system\", \"content\": round_2_response},\n",
    "            {\"role\": \"user\", \"content\": round_3_piano_tuners_explained},\n",
    "            {\"role\": \"system\", \"content\": round_3_response},\n",
    "            {\"role\": \"user\", \"content\": question_prompt}\n",
    "            ]\n",
    "    print(each_question)\n",
    "    prompt_token_count = num_tokens_from_messages(warmed_messages)\n",
    "    print(f\"\"\"No. of token used: {prompt_token_count}\"\"\")\n",
    "    try:\n",
    "        answer = apply_concept_recursion(warmed_messages, get_completion_from_messages(warmed_messages,max_tokens=4096-2*prompt_token_count))\n",
    "        print(answer)\n",
    "    except Exception as e:\n",
    "        answer = '''Error'''\n",
    "        answer_json = {\"error\": e}\n",
    "        python_message = '''\"\"\"python\\nstatus = \"error\"\\nprint(stats)\"\"\"'''\n",
    "        print(e)\n",
    "        row = [each_question, answer, answer_json, python_message]\n",
    "        df.loc[len(df)] = row\n",
    "        continue # next loop\n",
    "    print(f\"<<<<Completed Question {index} in question bank<<<<\")\n",
    "    try:\n",
    "        answer_json = text_to_json(answer)\n",
    "    except Exception as e:\n",
    "        answer_json = {\"error\": e}\n",
    "        print(e)\n",
    "    print(f\"<<<<Received Response to Question {index} <<<<\")\n",
    "    try:\n",
    "        python_message = text_to_python(answer)\n",
    "    except Exception as e:\n",
    "        python_message = '''\"\"\"python\\nstatus = \"error\"\\nprint(stats)\"\"\"'''\n",
    "        print(e)\n",
    "    print(f\"<<<<Received Python Code to Question {index} <<<<\")\n",
    "    # append the new data to the end of the dataframe\n",
    "    row = [each_question, answer, answer_json, python_message]\n",
    "    df.loc[len(df)] = row\n",
    "\n",
    "df.to_csv(f'market_sizing_high_temp_{timestamp}.csv', index=False)\n",
    "print(question_bank)\n",
    "print(answer)\n",
    "print(get_json(answer_json))\n",
    "print(get_python(python_message))\n",
    "\n",
    "## get_answer_step_by_step is about 35s per question\n",
    "## get_json is about 15s per question\n",
    "## get_python is about 25s per question\n",
    "# total = 75s per question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are approximately 12600 nurseries in Tokyo.\n",
      "Based on the assumptions and calculations, there are approximately 548 gas stations in Los Angeles.\n",
      "Based on the Fermi estimation, there could be around 4200 schools in Beijing.\n",
      "There are approximately 21600 nurseries in Mumbai.\n",
      "Using Fermi estimation, there are approximately 5220 barber shops in London.\n",
      "There are approximately 105 libraries in Paris.\n",
      "There might be around 10500 coffee shops in Beijing.\n",
      "Based on our assumptions, there are approximately 8000 coffee shops in New York City.\n",
      "There could be approximately 3523 hotels in Chicago based on these assumptions and calculations.\n",
      "An estimated 320 piano tuners are needed to service all the pianos in New York State.\n"
     ]
    }
   ],
   "source": [
    "# apply a function to the column \"answer_python\" for each row\n",
    "# df[df['name'].apply(lambda x: is_long(x) & is_short(x))]\n",
    "for index, row in df.iterrows():\n",
    "    # print(row['answer_python'])\n",
    "    exec(get_python(df['answer_python'][index]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('market_sizing_question.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec code directly will be dangerous, so we use exec to run the code in a sandbox\n",
    "#exec(get_python(python_message))\n",
    "# just print the message will do : )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices:\n",
    "# https://platform.openai.com/docs/guides/gpt-best-practices/strategy-split-complex-tasks-into-simpler-subtasks\n",
    "\n",
    "# Strategy: Split complex tasks into simpler subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Use a persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: User inner monologue\n",
    "# instruct the model to put parts of the output that are meant to be hidden from the user into a structured format\n",
    "# makes parsing them easy. \n",
    "\n",
    "# before presenting the output to the user, the output is parsed and only part of the output is made visible.\n",
    "\n",
    "system_message_IM_example = '''Follow these steps to answer the user queries.\n",
    "\n",
    "Step 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n",
    "\n",
    "Step 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n",
    "\n",
    "Step 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n",
    "\n",
    "Step 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
